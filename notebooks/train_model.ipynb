{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agent Core - 模型训练 Notebook\n",
    "\n",
    "本Notebook用于训练AI Agent所需的模型组件。\n",
    "\n",
    "## 训练目标\n",
    "- 推理模型 (Reasoning Model)\n",
    "- 嵌入模型 (Embedding Model)\n",
    "- 工具选择模型 (Tool Selection Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch版本: {torch.__version__}\")\n",
    "print(f\"CUDA可用: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA版本: {torch.version.cuda}\")\n",
    "print(f\"GPU数量: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU名称: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装依赖\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q transformers datasets accelerate peft bitsandbytes trl\n",
    "!pip install -q wandb tensorboard\n",
    "!pip install -q qdrant-client pgvector psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入库\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置随机种子\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"环境设置完成!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练配置\n",
    "CONFIG = {\n",
    "    # 模型配置\n",
    "    \"model_name\": \"Qwen/Qwen2.5-7B-Instruct\",  # 可替换为其他模型\n",
    "    \"max_length\": 2048,\n",
    "    \"vocab_size\": None,  # 自动获取\n",
    "    \n",
    "    # LoRA配置\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    \n",
    "    # 训练配置\n",
    "    \"output_dir\": \"./outputs\",\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"per_device_eval_batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"logging_steps\": 10,\n",
    "    \"save_steps\": 500,\n",
    "    \"eval_steps\": 500,\n",
    "    \"save_total_limit\": 3,\n",
    "    \n",
    "    # 量化配置\n",
    "    \"use_4bit\": True,\n",
    "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
    "    \"bnb_4bit_quant_type\": \"nf4\",\n",
    "    \"bnb_4bit_use_double_quant\": True,\n",
    "}\n",
    "\n",
    "# 创建输出目录\n",
    "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "# 保存配置\n",
    "with open(os.path.join(CONFIG[\"output_dir\"], \"config.json\"), \"w\") as f:\n",
    "    json.dump(CONFIG, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"配置参数:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent训练数据格式\n",
    "TRAIN_DATA_EXAMPLE = [\n",
    "    {\n",
    "        \"instruction\": \"分析用户请求并决定下一步行动\",\n",
    "        \"input\": \"用户想查询北京明天的天气\",\n",
    "        \"output\": \"{\\\"thought\\\": \\\"用户需要天气信息，我需要调用天气查询工具\\\", \\\"action\\\": \\\"weather_query\\\", \\\"action_input\\\": {\\\"city\\\": \\\"北京\\\", \\\"date\\\": \\\"明天\\\"}}\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"根据工具执行结果生成最终回答\",\n",
    "        \"input\": \"工具返回: 北京明天晴天，温度15-25°C\",\n",
    "        \"output\": \"{\\\"thought\\\": \\\"已获取天气信息，可以给出最终答案\\\", \\\"action\\\": \\\"final_answer\\\", \\\"action_input\\\": {\\\"answer\\\": \\\"北京明天天气晴朗，温度在15到25摄氏度之间，适合外出活动。\\\"}}\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 创建示例数据集\n",
    "def create_sample_dataset(size: int = 100) -> List[Dict]:\n",
    "    \"\"\"创建示例训练数据集\"\"\"\n",
    "    data = []\n",
    "    templates = [\n",
    "        {\"task\": \"查询天气\", \"tool\": \"weather_query\", \"params\": [\"city\", \"date\"]},\n",
    "        {\"task\": \"计算数学表达式\", \"tool\": \"calculator\", \"params\": [\"expression\"]},\n",
    "        {\"task\": \"搜索信息\", \"tool\": \"search\", \"params\": [\"query\"]},\n",
    "        {\"task\": \"获取时间\", \"tool\": \"datetime\", \"params\": [\"format\"]},\n",
    "    ]\n",
    "    \n",
    "    cities = [\"北京\", \"上海\", \"广州\", \"深圳\", \"杭州\"]\n",
    "    \n",
    "    for i in range(size):\n",
    "        template = templates[i % len(templates)]\n",
    "        \n",
    "        if template[\"tool\"] == \"weather_query\":\n",
    "            city = cities[i % len(cities)]\n",
    "            data.append({\n",
    "                \"instruction\": \"分析用户请求并决定下一步行动\",\n",
    "                \"input\": f\"用户想查询{city}的天气情况\",\n",
    "                \"output\": json.dumps({\n",
    "                    \"thought\": f\"用户需要{city}的天气信息，调用天气查询工具\",\n",
    "                    \"action\": \"weather_query\",\n",
    "                    \"action_input\": {\"city\": city}\n",
    "                }, ensure_ascii=False)\n",
    "            })\n",
    "        elif template[\"tool\"] == \"calculator\":\n",
    "            expr = f\"{i * 2} + {i * 3}\"\n",
    "            data.append({\n",
    "                \"instruction\": \"分析用户请求并决定下一步行动\",\n",
    "                \"input\": f\"计算 {expr}\",\n",
    "                \"output\": json.dumps({\n",
    "                    \"thought\": \"用户需要进行数学计算，调用计算器工具\",\n",
    "                    \"action\": \"calculator\",\n",
    "                    \"action_input\": {\"expression\": expr}\n",
    "                }, ensure_ascii=False)\n",
    "            })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 生成数据\n",
    "train_data = create_sample_dataset(1000)\n",
    "eval_data = create_sample_dataset(200)\n",
    "\n",
    "print(f\"训练数据: {len(train_data)} 条\")\n",
    "print(f\"验证数据: {len(eval_data)} 条\")\n",
    "print(f\"\\n示例数据:\")\n",
    "print(json.dumps(train_data[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 格式化数据为训练格式\n",
    "def format_data(example: Dict) -> str:\n",
    "    \"\"\"将数据格式化为模型输入格式\"\"\"\n",
    "    return f\"\"\"<|im_start|>system\n",
    "你是一个智能AI助手，能够分析用户请求并选择合适的工具执行任务。\n",
    "你的输出必须是JSON格式：{{\"thought\": \"思考过程\", \"action\": \"工具名或final_answer\", \"action_input\": {{参数}}}}\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "{example['instruction']}\n",
    "\n",
    "{example['input']}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{example['output']}<|im_end|>\"\"\"\n",
    "\n",
    "# 创建Dataset\n",
    "train_dataset = Dataset.from_list([\n",
    "    {\"text\": format_data(d)} for d in train_data\n",
    "])\n",
    "eval_dataset = Dataset.from_list([\n",
    "    {\"text\": format_data(d)} for d in eval_data\n",
    "])\n",
    "\n",
    "print(\"数据集创建完成!\")\n",
    "print(f\"\\n格式化示例:\")\n",
    "print(train_dataset[0]['text'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    trust_remote_code=True,\n",
    "    padding_side='right'\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer加载完成\")\n",
    "print(f\"词表大小: {len(tokenizer)}\")\n",
    "print(f\"PAD Token: {tokenizer.pad_token}\")\n",
    "print(f\"EOS Token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 量化配置\n",
    "bnb_config = None\n",
    "if CONFIG[\"use_4bit\"]:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=CONFIG[\"bnb_4bit_quant_type\"],\n",
    "        bnb_4bit_compute_dtype=getattr(torch, CONFIG[\"bnb_4bit_compute_dtype\"]),\n",
    "        bnb_4bit_use_double_quant=CONFIG[\"bnb_4bit_use_double_quant\"],\n",
    "    )\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "print(f\"模型加载完成: {CONFIG['model_name']}\")\n",
    "print(f\"模型参数量: {model.num_parameters() / 1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA配置\n",
    "if CONFIG[\"use_4bit\"]:\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    target_modules=CONFIG[\"target_modules\"],\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\nLoRA配置完成!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    num_train_epochs=CONFIG[\"num_train_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"per_device_eval_batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    lr_scheduler_type=CONFIG[\"lr_scheduler_type\"],\n",
    "    logging_steps=CONFIG[\"logging_steps\"],\n",
    "    save_steps=CONFIG[\"save_steps\"],\n",
    "    eval_steps=CONFIG[\"eval_steps\"],\n",
    "    save_total_limit=CONFIG[\"save_total_limit\"],\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    run_name=f\"agent-training-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    ")\n",
    "\n",
    "print(\"训练参数配置完成!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=CONFIG[\"max_length\"],\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"Trainer创建完成!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始训练\n",
    "print(\"=\"*50)\n",
    "print(\"开始训练...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"训练完成!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\n训练损失: {train_result.training_loss:.4f}\")\n",
    "print(f\"训练步数: {train_result.global_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存LoRA权重\n",
    "lora_output_dir = os.path.join(CONFIG[\"output_dir\"], \"lora_weights\")\n",
    "trainer.model.save_pretrained(lora_output_dir)\n",
    "tokenizer.save_pretrained(lora_output_dir)\n",
    "\n",
    "print(f\"LoRA权重已保存到: {lora_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并LoRA权重并保存完整模型\n",
    "from peft import PeftModel\n",
    "\n",
    "# 重新加载基础模型\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# 加载LoRA权重\n",
    "merged_model = PeftModel.from_pretrained(base_model, lora_output_dir)\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "# 保存合并后的模型\n",
    "merged_output_dir = os.path.join(CONFIG[\"output_dir\"], \"merged_model\")\n",
    "merged_model.save_pretrained(merged_output_dir)\n",
    "tokenizer.save_pretrained(merged_output_dir)\n",
    "\n",
    "print(f\"合并模型已保存到: {merged_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试推理\n",
    "def generate_response(model, tokenizer, prompt: str, max_new_tokens: int = 256) -> str:\n",
    "    \"\"\"生成模型响应\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# 测试用例\n",
    "test_prompts = [\n",
    "    \"\"\"<|im_start|>system\n",
    "你是一个智能AI助手，能够分析用户请求并选择合适的工具执行任务。\n",
    "你的输出必须是JSON格式：{\"thought\": \"思考过程\", \"action\": \"工具名或final_answer\", \"action_input\": {参数}}\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "分析用户请求并决定下一步行动\n",
    "\n",
    "用户想查询上海今天的天气<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"模型测试\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\n输入: {prompt.split('<|im_start|>user\\n')[1].split('<|im_end|>')[0][:100]}...\")\n",
    "    response = generate_response(merged_model, tokenizer, prompt)\n",
    "    print(f\"\\n输出: {response}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 上传到Hugging Face (可选)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上传到Hugging Face\n",
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "# 登录 (需要先设置HF_TOKEN环境变量或在Colab中使用huggingface-cli login)\n",
    "# login(token=\"your_hf_token\")\n",
    "\n",
    "# 上传模型\n",
    "HF_REPO_ID = \"your-username/your-model-name\"  # 修改为你的仓库ID\n",
    "\n",
    "def upload_to_hf(model_path: str, repo_id: str):\n",
    "    \"\"\"上传模型到Hugging Face\"\"\"\n",
    "    api = HfApi()\n",
    "    \n",
    "    try:\n",
    "        api.create_repo(repo_id=repo_id, exist_ok=True)\n",
    "        api.upload_folder(\n",
    "            folder_path=model_path,\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"model\",\n",
    "        )\n",
    "        print(f\"模型已上传到: https://huggingface.co/{repo_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"上传失败: {e}\")\n",
    "\n",
    "# 取消注释以执行上传\n",
    "# upload_to_hf(merged_output_dir, HF_REPO_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 下载模型到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打包模型用于下载\n",
    "import shutil\n",
    "\n",
    "zip_path = f\"{CONFIG['output_dir']}/model_archive\"\n",
    "shutil.make_archive(zip_path, 'zip', merged_output_dir)\n",
    "\n",
    "print(f\"模型已打包: {zip_path}.zip\")\n",
    "print(f\"文件大小: {os.path.getsize(zip_path + '.zip') / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从Colab下载\n",
    "from google.colab import files\n",
    "\n",
    "files.download(f\"{zip_path}.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 训练总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练总结\n",
    "summary = {\n",
    "    \"model_name\": CONFIG[\"model_name\"],\n",
    "    \"training_epochs\": CONFIG[\"num_train_epochs\"],\n",
    "    \"train_loss\": train_result.training_loss,\n",
    "    \"global_step\": train_result.global_step,\n",
    "    \"lora_r\": CONFIG[\"lora_r\"],\n",
    "    \"learning_rate\": CONFIG[\"learning_rate\"],\n",
    "    \"output_dir\": CONFIG[\"output_dir\"],\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "}\n",
    "\n",
    "# 保存总结\n",
    "with open(os.path.join(CONFIG[\"output_dir\"], \"training_summary.json\"), \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"训练总结\")\n",
    "print(\"=\"*50)\n",
    "for k, v in summary.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
